# Agentic AI PoC - Environment Configuration
# Copy this file to .env and fill in your actual values

# =============================================================================
# Model Configuration
# =============================================================================

# LiteLLM model to use (e.g., ollama/llama3, gpt-4-turbo-preview, claude-3-opus)
LITELLM_MODEL=ollama/llama3

# OpenAI API key (only required for OpenAI models)
OPENAI_API_KEY=

# Ollama base URL (required for local Ollama models)
OLLAMA_BASE_URL=http://127.0.0.1:11434

# =============================================================================
# Integration API Keys
# =============================================================================

# Linear API key (required for Linear integration)
LINEAR_API_KEY=

# Linear webhook secret (required for webhook signature verification)
LINEAR_WEBHOOK_SECRET=

# GitHub token (optional, for knowledge base ingestion)
GITHUB_TOKEN=

# Notion token (optional, for knowledge base ingestion)
NOTION_TOKEN=

# Jira API token (optional, for knowledge base ingestion)
JIRA_TOKEN=

# Jira user email (required for Atlassian Cloud Basic Auth)
JIRA_USER_EMAIL=

# Jira base URL (e.g., https://your-domain.atlassian.net)
JIRA_BASE_URL=

# Confluence API token (optional, for knowledge base ingestion)
CONFLUENCE_TOKEN=

# Confluence user email (required for Atlassian Cloud Basic Auth)
CONFLUENCE_USER_EMAIL=

# Confluence base URL (e.g., https://your-domain.atlassian.net/wiki)
CONFLUENCE_BASE_URL=

# =============================================================================
# Integration Targets
# =============================================================================

# GitHub repository to ingest (format: owner/repo)
GITHUB_REPO=

# Notion root page ID to ingest
NOTION_ROOT_PAGE_ID=

# Linear team ID to work with
LINEAR_TEAM_ID=

# Jira project keys to ingest (comma-separated)
JIRA_PROJECT_KEYS=

# Confluence space keys to ingest (comma-separated)
CONFLUENCE_SPACE_KEYS=

# =============================================================================
# Deployment Mode
# =============================================================================

# Dry run mode: if true, no changes will be made to external systems
DRY_RUN=true

# Execution mode: shadow|comment_only|autonomous
# - shadow: Only log what would be done
# - comment_only: Add comments but don't modify issues
# - autonomous: Fully automated updates
MODE=comment_only

# Label required for autonomous mode to apply changes
REQUIRE_APPROVAL_LABEL=ai-refined

# =============================================================================
# Vector Store Configuration
# =============================================================================

# Path to LanceDB vector store directory
VECTOR_STORE_PATH=./data/lancedb

# Embedding model to use for vector embeddings
EMBEDDING_MODEL=local/all-MiniLM-L6-v2

# =============================================================================
# Message Queue (Redis)
# =============================================================================

# Redis connection URL for message queue
REDIS_URL=redis://localhost:6379/0

# =============================================================================
# Observability
# =============================================================================

# OpenTelemetry OTLP exporter endpoint
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# Enable OpenTelemetry tracing
ENABLE_TRACING=true
